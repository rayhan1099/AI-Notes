- nontechnical reading: https://www.oneusefulthing.org/p/on-giving-ai-eyes-and-ears
- https://huyenchip.com/2023/10/10/multimodal.html
	- [CLIP](https://arxiv.org/abs/2103.00020) (2021) and [Flamingo](https://arxiv.org/abs/2204.14198) (2022) both for their significance as well as availability and clarity of public details.
	-   CLIP was the first model that could generalize to multiple **image classification tasks** with zero- and few-shot learning.
	-   Flamingo wasn’t the first large multimodal model that could **generate open-ended responses** ([Salesforce’s BLIP](https://arxiv.org/abs/2201.12086) came out 3 months prior). However, Flamingo’s strong performance prompted some to consider it [the GPT-3 moment in the multimodal domain](https://arxiv.org/abs/2304.08485).
- What is CLIP? CLIP is the first multimodal (in this case, vision and text) model tackling computer vision https://www.kdnuggets.com/2021/03/beginners-guide-clip-model.html
- DeepMind announced [Gato](https://www.deeplearning.ai/the-batch/one-model-hundreds-of-tasks/), a transformer that It learned over 600 diverse tasks — playing Atari games, stacking blocks using a robot arm, generating image captions, and so on — though not necessarily as well as separate models dedicated to those tasks. The system underwent supervised training on a wide variety of datasets simultaneously, from text and images to actions generated by reinforcement learning agents.
-   As the year drew to a close, researchers at Google brought a similar range of abilities to robotics. [RT-1](https://ai.googleblog.com/2022/12/rt-1-robotics-transformer-for-real.html?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8HbXG-ZkwAj82Nv49uUrBwOHz4zUj3mkyjIfEd5lU7h3JHZR0pEG5OpkUCPPqwWvqMbjWl) is a transformer that enables robots to perform over 700 tasks. The system, which tokenizes actions as well as images, learned from a dataset of 130,000 episodes collected from a fleet of robots over nearly a year and a half. It achieved outstanding zero-shot performance in new tasks, environments, and objects compared to prior techniques.
- Palm E robotics and vision demo https://twitter.com/dannydriess/status/1632904675124035585?s=46&t=90xQ8sGy63D2OtiaoGJuww
- google MUM https://blog.google/products/search/introducing-mum/
- "this is not a pipe" really is a pipe
- Adept Fuyu 8B https://www.adept.ai/blog/fuyu-8b
	- It's noteworthy that Fuyu passes the input patches directly into a linear projection (or embedding layer) to learn its own image patch embeddings rather than relying on an additional pretrained image encoder like other models and methods do (examples include [LLaVA](https://github.com/haotian-liu/LLaVA) and [MiniGPT-V](https://github.com/Vision-CAIR/MiniGPT-4). This greatly simplifies the architecture and training setup.
- Flamingo models take advantage of two complementary  models: a vision model that analyzes visual scenes and a large language model   which performs a basic form of reasoning. The language model is trained on a  large amount of text data. https://arxiv.org/pdf/2301.04655.pdf
	- independent reproductions https://twitter.com/sanhestpasmoi/status/1632775840135016448?s=46&t=90xQ8sGy63D2OtiaoGJuww
	- Openflamingo https://laion.ai/blog/open-flamingo/  an open-source reproduction of DeepMind's Flamingo model. At its core, OpenFlamingo is a framework that enables training and evaluation of large multimodal models (LMMs).
	- The Multimodal-C4 dataset is an expansion of the text-only [C4 dataset](https://www.tensorflow.org/datasets/catalog/c4), which was used to train [T5 models](https://arxiv.org/abs/1910.10683). For each document in the [C4 en.clean](https://www.tensorflow.org/datasets/catalog/c4#c4en_default_config) dataset, we retrieve the original webpage from [Common Crawl](https://commoncrawl.org/), then collect the downloadable images. Data cleaning is carried out through deduplication and content filtering, which aims to eliminate non-safe for work (NSFW) and unrelated images, such as advertisements. Additionally, we run face detection and discard images with positive identifications. Finally, images and sentences are interleaved using bipartite matching within a document: CLIP ViT/L-14 image-text similarities serve as edge weights. Multimodal-C4 consists of approximately 75 million documents, encompassing around 400M images and 38B tokens. 
- MiniGPT-4 https://minigpt-4.github.io/
	- On a technical level, they're doing something really simple -- take BLIP2's ViT-L+Q-former, connect it to Vicuna-13B with a linear layer, and train just the tiny layer on some datasets of image-text pairs.
- Meta-Transformer https://kxgong.github.io/meta_transformer/
	- ![https://kxgong.github.io/meta_transformer/static/images/teaser.png](https://kxgong.github.io/meta_transformer/static/images/teaser.png)

microsoft
- we introduce a new computer vision foundation model, Florence, to expand the representations from coarse (scene) to fine (object), from static (images) to dynamic (videos), and from RGB to multiple modalities (caption, depth). By incorporating universal visual-language representations from Web-scale image-text data, our Florence model can be easily adapted for various computer vision tasks, such as classification, retrieval, object detection, VQA, image caption, video retrieval and action recognition.

google multimodal researchc https://ai.googleblog.com/2023/01/google-research-2022-beyond-language.html?m=1


visual chatgpt https://github.com/microsoft/visual-chatgpt


hugging gpt - chatgpt coordinating between huggingface models https://arxiv.org/abs/2303.17580
  
https://news.ycombinator.com/item?id=35764355
We are building EVA, an AI-Relational database system with first-class support for deep learning models. Our goal with EVA is to create a platform that supports AI-powered multi-modal database applications operating on structured (tables, feature vectors, etc.) and unstructured data (videos, podcasts, pdf, etc.) with deep learning models. EVA comes with a wide range of models for analyzing unstructured data, including models for object detection, OCR, text summarization, audio speech recognition, and more.

https://text-generator.io/blog/embed-images-text-and-code Embed Images, Text and Code in the same space

https://www.theverge.com/2023/5/9/23716558/meta-imagebind-open-source-multisensory-modal-ai-model-research The new ImageBind model combines text, audio, visual, movement, thermal, and depth data.


Otter https://twitter.com/liuziwei7/status/1652685051702960128
*, a multi-modal in-context learning model with instruction tuning: 1) Chatbot w/ image, video, 3D 2) Only need 4x 3090 GPUs 3) Better than OpenFlamingo - Code: [https://github.com/Luodian/otter](https://t.co/Rx5hT53eu6) - Demo: [https://otter.cliangyu.com](https://t.co/xi3mWsBOVv) - Video: [https://youtube.com/watch?v=r-YM4DGGAdE](https://t.co/NR7rWVFlAJ)


Gato model - 1.2b param model

### GPT4 multimodal 
https://twitter.com/DrJimFan/status/1634244545360609289

Shameless plug -- our recent model FROMAGe is one step towards enabling existing LLMs to use images as both inputs and outputs: [https://jykoh.com/fromage](https://t.co/LTEqt2kNvi)

Announcing Genmo Chat, a creative copilot that uses GPT-4 and a large suite of generative AI tools to create and then edit any video or image you ask for. ([tweet](https://twitter.com/genmoai/status/1640760678167478274))

